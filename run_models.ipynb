{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from create_word_vectors import * \n",
    "import pickle \n",
    "import helper\n",
    "from tfidf_methods import * \n",
    "from paths import * \n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare glove embedding mazrix and save it in embeddings\n",
    "X_sequences, y, Kgl_sequences, nb_word , word_embedding, glove_matrix = run_glove_embbedding(GLOVE_EMBEDDING\n",
    "                                        + \"embeddings.pkl\", n_grams = 1, pretrained = True, max_words = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_train, y, Kgl_seuquences, nb_word , word_embedding, glove_matrix from the glove file\n",
    "[X_sequences, y, Kgl_sequences, nb_word , word_embedding, glove_matrix] = pickle.load(open(GLOVE_EMBEDDING + \"emebddings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the following values to train models on google colab cluster\n",
    "#pickle.dump([X_sequences, y, Kgl_sequences,nb_word, glove_matrix], open('cluster_2ngrams.pkl', 'wb'),protocol=2)\n",
    "\n",
    "\n",
    "'''\n",
    "import gzip\n",
    "import pickle\n",
    "fp=gzip.open('cluster.pkl.gz','rb')\n",
    "[X_sequences, y, Kgl_sequences,nb_word, glove_matrix]=pickle.load(fp)\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "fp=gzip.open('cluster.pkl.gz','rb')\n",
    "[X_sequences, y, Kgl_sequences,nb_word, glove_matrix]=pickle.load(fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned files! we will not always perform \n",
    "X = pickle.load(open(CLEANED_DATA_PATH + 'X_Cleaned.pkl', \"rb\"))\n",
    "X_Kgl = pickle.load(open(CLEANED_DATA_PATH + 'X_Kgl_Cleaned.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tfidf embedding\n",
    "#X_train_tfidf_avg, X_test_tfidf_avg, X_train_tfidf, X_test_tfidf = tfidf_embedding(X, X_Kgl, word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_tfidf_avg, y, 32,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MODELS = 'models_saving/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisrt model\n",
    "np.random.seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "train_model_1 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_model_1 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_model_1, open(DATA_MODELS + 'train_model_1', 'wb'))\n",
    "pickle.dump(test_model_1, open(DATA_MODELS + 'test_model_1', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second model: Embedding + Convolution + MaxPooling+LSTM+ Dense layer with sigmoid activation\n",
    "np.random.seed(2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "train_model_2 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_model_2 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_model_2, open(DATA_MODELS + 'train_model_2', 'wb'))\n",
    "pickle.dump(test_model_2, open(DATA_MODELS + 'test_model_2', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "# third model\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1,200, input_length = X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "train_model_3 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_model_3 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_model_3, open(DATA_MODELS + 'train_model_3', 'wb'))\n",
    "pickle.dump(test_model_3, open(DATA_MODELS + 'test_model_3', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "\n",
    "train_4 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_4 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_4, open(DATA_MODELS + 'train_model_4', 'wb'))\n",
    "pickle.dump(test_4, open(DATA_MODELS + 'test_model_4', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1, 200, input_length=X_sequences.shape[1]))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1)\n",
    "\n",
    "train_5 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_5 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_5, open(DATA_MODELS + 'train_model_5', 'wb'))\n",
    "pickle.dump(test_5, open(DATA_MODELS + 'test_model_5', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 50, input_length=X_sequences.shape[1]))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "\n",
    "train_6 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_6 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_6, open(DATA_MODELS + 'train_model_6', 'wb'))\n",
    "pickle.dump(test_6, open(DATA_MODELS + 'test_model_6', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 50, input_length=X_sequences.shape[1]))\n",
    "model.add(Convolution1D(filters=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "\n",
    "train_7 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_7 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_7, open(DATA_MODELS + 'train_model_7', 'wb'))\n",
    "pickle.dump(test_7, open(DATA_MODELS + 'test_model_7', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 50, input_length=X_sequences.shape[1]))\n",
    "model.add(Convolution1D(filters=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "\n",
    "train_8 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_8 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_8,  open(DATA_MODELS + 'train_model_8', 'wb'))\n",
    "pickle.dump(test_8, open(DATA_MODELS + 'test_model_8', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_tfidf_avg, y, epochs=3, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisrt model\n",
    "np.random.seed(9)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "train_model_9 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_model_9 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_model_9, open(DATA_MODELS + 'train_model_9', 'wb'))\n",
    "pickle.dump(test_model_9, open(DATA_MODELS + 'test_model_9', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisrt model\n",
    "np.random.seed(10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "train_model_10 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_model_10 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_model_10, open(DATA_MODELS + 'train_model_10', 'wb'))\n",
    "pickle.dump(test_model_10, open(DATA_MODELS + 'test_model_10', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GRU\n",
    "np.random.seed(11)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(GRU(units=16, name = \"gru_1\",return_sequences=True))\n",
    "model.add(GRU(units=8, name = \"gru_2\" ,return_sequences=True))\n",
    "model.add(GRU(units=4, name= \"gru_3\"))\n",
    "model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=1, batch_size=128)\n",
    "\n",
    "train_11 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_11 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_11,  open(DATA_MODELS + 'train_model_11', 'wb'))\n",
    "pickle.dump(test_11, open(DATA_MODELS + 'test_model_11', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 200, input_length=X_sequences.shape[1], weights=[glove_matrix]))\n",
    "model.add(GRU(100, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "\n",
    "train_12 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_12 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_12, open(DATA_MODELS + 'train_model_12', 'wb'))\n",
    "pickle.dump(test_12, open(DATA_MODELS + 'test_model_12', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_word+1, 50, input_length=X_sequences.shape[1]))\n",
    "model.add(Convolution1D(filters=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_sequences, y, validation_split=0.1, epochs=2, batch_size=128, verbose=1, shuffle=True)\n",
    "\n",
    "train_13 = model.predict_proba(X_sequences, batch_size=128)\n",
    "test_13 = model.predict_proba(Kgl_sequences)\n",
    "\n",
    "pickle.dump(train_13,  open(DATA_MODELS + 'train_model_13', 'wb'))\n",
    "pickle.dump(test_13, open(DATA_MODELS + 'test_model_13', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
